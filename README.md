# ğŸ“ Grammar Correction using FLAN-T5 with LoRA Fine-Tuning

This project implements a **grammar correction system** using the **FLAN-T5-base transformer** model, fine-tuned with **LoRA (Low-Rank Adaptation)**. It is built for efficiency and performance â€” covering data preprocessing, model fine-tuning, evaluation, deployment using Streamlit, and containerization with Docker.

---

## ğŸ“ Dataset Preparation

The dataset consists of pairs of **ungrammatical input sentences** and their corresponding **grammatically correct outputs**.

**Example:**

| Input                                           | Output                                             |
|------------------------------------------------|----------------------------------------------------|
| correct grammar: double story bed curtain around | A double story bed with curtains all around.       |
| correct grammar: cat lay on floor paw in shoe   | A cat is laying on the floor with its paw in a shoe|

- The prefix `correct grammar:` is added to instruct the FLAN-T5 model.
- The dataset was cleaned and converted into a CSV format.
- Loaded using the Hugging Face `datasets` library and tokenized using the `AutoTokenizer` from `transformers`.

---

## ğŸ§  Model Architecture and Fine-Tuning

### âœ… Base Model:
- [`google/flan-t5-base`](https://huggingface.co/google/flan-t5-base)

### âœ… Fine-Tuning Strategy:
- **LoRA (Low-Rank Adaptation)** is used to perform parameter-efficient fine-tuning.
- This avoids updating all weights, saving compute and memory while maintaining strong performance.

### âœ… Libraries Used:
- `transformers`, `peft`, `datasets`, `torch`, `evaluate`, `sentencepiece`, `scikit-learn`

### Training Summary:
- 2 epochs
- Batch size: 8
- Learning rate: 2e-4
- Trained using Hugging Faceâ€™s `Seq2SeqTrainer`
- Loss decreased across epochs indicating successful learning

---

## ğŸ“Š Evaluation Metrics

The model is evaluated on the following NLP metrics:

| Metric   | Score       |
|----------|-------------|
| **BLEU** | 0.71â€“0.72   |
| **ROUGE-L** | ~0.90   |
| **METEOR** | ~0.91     |
| **chrF++** | ~86.13    |

> These are **state-of-the-art level scores** for a lightweight model with just two epochs of training using LoRA!

---

## ğŸŒ Streamlit App Deployment

An interactive **Streamlit web app** is created for live grammar correction.

### Features:
- Enter any ungrammatical sentence
- Get the corrected version instantly
- Clean and responsive UI

### Run Locally:
```bash
# Activate your Python virtual environment
streamlit run app.py


ğŸ³ Docker Containerization
This project includes Docker support for easy deployment.

Dockerfile highlights:
Uses python:3.10 as base image

Copies project files

Installs dependencies from requirements.txt

Exposes port 8501 for Streamlit

Build and Run:
bash
Copy
Edit
# Build the Docker image
docker build -t vardan201/grammarcorrection .

# Run the container
docker run -p 8501:8501 vardan201/grammarcorrection

grammar-correction/
â”œâ”€â”€ app.py                    # Streamlit app
â”œâ”€â”€ model/                    # LoRA fine-tuned model
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ Dockerfile                # Docker setup
â”œâ”€â”€ dataset.csv               # Input-output sentence pairs
â”œâ”€â”€ Grammar_Correction_model.py            # Training script
â””â”€â”€ README.md                 # Project documentation


ğŸ“Œ requirements.txt (with Pinned Versions)
ini
Copy
Edit
transformers==4.52.4
peft==0.15.2
accelerate==1.8.1
datasets==3.6.0
torch==2.7.1
sentencepiece==0.2.0
evaluate==0.4.1
scikit-learn==1.7.0
streamlit==1.45.1

ğŸ› ï¸ How to Rebuild This Project
Clone the repository

Prepare a dataset with ungrammatical â†’ grammatical sentence pairs

Tokenize the data using FLAN-T5 tokenizer

Apply LoRA and fine-tune the FLAN-T5 model

Evaluate the model using BLEU, ROUGE-L, METEOR, and chrF++

Deploy with Streamlit locally or using Docker

ğŸ™Œ Final Notes
This project combines efficient fine-tuning (LoRA) with powerful instruction-tuned models (FLAN-T5).

The system delivers high performance with low compute needs.

Fully deployable via Streamlit and Docker â€” portable across environments.
